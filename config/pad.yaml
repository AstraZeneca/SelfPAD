---
seed: 57
use_wandb: False
use_validation: False
root_path: ./
data_path: ./data

# Data related options
y_label: targets
vocab_size: 10261
p_vocab: 0.2      # Proportion of the vocabulary to sample in each iteration
num_labels: 100   # Number of data points sampled. Keep it small
pad_index: 23     # Index of mask token in the vocabulary

# Model architecutre
max_seq_length: 180    # maximum sequence length
model_dim: 32          # Embedding dim of the output of each Att block
n_embd: 32             # == model_dim
dim_head: 16           # Embedding dim of each attention head
heads: 4               # Number of attention heads
depth: 4               # Number of att layers
mult: 4                # multiplier used to scale the embedding dimension within feed-forward layer
dim_o: 128             # Embedding dim of the encoder

# Data splits
training_data_ratio: 1.0  # Training vs valiation. 1.0==All is used for training i.e. no validation set
                          # Test set is provided seperately

# Hyper-parameters. If hard stop is defined, the training ends either when we reach max_epochs or hard_stop
max_epochs: 1000                  # Maximum number of epochs
hard_stop: 90000                  # Maximum number of iterations
learning_rate: 0.001              # Learning rate for training
dropout_rate: 0.2                 # Set dropout rate if Dropout is being used
tau: 0.1                          # temperature parameter used in NTXentLoss
batch_size: 2                     # Set batch size
scheduler: false                  # If true, it will use scheduler for learning rate
val_check_interval: 100           # Intervals between each validation checks

# Normalisation and Objective func.
cosine_similarity: False          # If True, use cosine similarity in NTXentLoss. Else, use dot product.
p_norm: 2                         # p-value used for normalization. p=2 for L2 norm, p=1 for L1 norm and so on.


# Losses to use
contrastive_loss: true
distance_loss: False

# Wheter to add noise
add_noise: true